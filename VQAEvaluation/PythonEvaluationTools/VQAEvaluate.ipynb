{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'vqaEvaluation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5f35dfd95244>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%s/PythonHelperTools/vqaTools'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataDir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvqa\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvqaEvaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvqaEval\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVQAEval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'vqaEvaluation'"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import sys\n",
    "dataDir = '/Users/yanan/Desktop/VQAEvaluation'\n",
    "sys.path.insert(0, '%s/PythonHelperTools/vqaTools' %(dataDir))\n",
    "from vqa import VQA\n",
    "from vqaEvaluation.vqaEval import VQAEval\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# set up file names and paths\n",
    "versionType ='' # this should be '' when using VQA v2.0 dataset\n",
    "taskType    ='OpenEnded' # 'OpenEnded' only for v2.0. 'OpenEnded' or 'MultipleChoice' for v1.0\n",
    "dataType    ='mscoco'  # 'mscoco' only for v1.0. 'mscoco' for real and 'abstract_v002' for abstract for v1.0. \n",
    "dataSubType ='train2014'\n",
    "annFile     ='%s/Annotations/%s%s_%s_annotations.json'%(dataDir, versionType, dataType, dataSubType)\n",
    "quesFile    ='%s/Questions/%s%s_%s_%s_questions.json'%(dataDir, versionType, taskType, dataType, dataSubType)\n",
    "#imgDir      ='%s/Images/%s/%s/' %(dataDir, dataType, dataSubType)\n",
    "resultType  ='fake'\n",
    "fileTypes   = ['results', 'accuracy', 'evalQA', 'evalQuesType', 'evalAnsType'] \n",
    "\n",
    "# An example result json file has been provided in './Results' folder.  \n",
    "\n",
    "[resFile, accuracyFile, evalQAFile, evalQuesTypeFile, evalAnsTypeFile] = ['%s/Results/%s%s_%s_%s_%s_%s.json'%(dataDir, versionType, taskType, dataType, dataSubType, \\\n",
    "resultType, fileType) for fileType in fileTypes]  \n",
    "\n",
    "# create vqa object and vqaRes object\n",
    "vqa = VQA(annFile, quesFile)\n",
    "vqaRes = vqa.loadRes(resFile, quesFile)\n",
    "\n",
    "# create vqaEval object by taking vqa and vqaRes\n",
    "vqaEval = VQAEval(vqa, vqaRes, n=2)   #n is precision of accuracy (number of places after decimal), default is 2\n",
    "\n",
    "# evaluate results\n",
    "\"\"\"\n",
    "If you have a list of question ids on which you would like to evaluate your results, pass it as a list to below function\n",
    "By default it uses all the question ids in annotation file\n",
    "\"\"\"\n",
    "vqaEval.evaluate() \n",
    "\n",
    "# print accuracies\n",
    "print(\"\\n\")\n",
    "print(\"Overall Accuracy is: %.02f\\n\" %(vqaEval.accuracy['overall']))\n",
    "print(\"Per Question Type Accuracy is the following:\")\n",
    "for quesType in vqaEval.accuracy['perQuestionType']:\n",
    "\tprint(\"%s : %.02f\" %(quesType, vqaEval.accuracy['perQuestionType'][quesType]))\n",
    "print(\"\\n\")\n",
    "print(\"Per Answer Type Accuracy is the following:\")\n",
    "for ansType in vqaEval.accuracy['perAnswerType']:\n",
    "\tprint(\"%s : %.02f\" %(ansType, vqaEval.accuracy['perAnswerType'][ansType]))\n",
    "print(\"\\n\")\n",
    "# demo how to use evalQA to retrieve low score result\n",
    "evals = [quesId for quesId in vqaEval.evalQA if vqaEval.evalQA[quesId]<35]   #35 is per question percentage accuracy\n",
    "if len(evals) > 0:\n",
    "\tprint('ground truth answers')\n",
    "\trandomEval = random.choice(evals)\n",
    "\trandomAnn = vqa.loadQA(randomEval)\n",
    "\tvqa.showQA(randomAnn)\n",
    "\n",
    "\tprint('\\n')\n",
    "\tprint('generated answer (accuracy %.02f)'%(vqaEval.evalQA[randomEval]))\n",
    "\tann = vqaRes.loadQA(randomEval)[0]\n",
    "\tprint(\"Answer:   %s\\n\" %(ann['answer']))\n",
    "\n",
    "\timgId = randomAnn[0]['image_id']\n",
    "\timgFilename = 'COCO_' + dataSubType + '_'+ str(imgId).zfill(12) + '.jpg'\n",
    "\tif os.path.isfile(imgDir + imgFilename):\n",
    "\t\tI = io.imread(imgDir + imgFilename)\n",
    "\t\tplt.imshow(I)\n",
    "\t\tplt.axis('off')\n",
    "\t\tplt.show()\n",
    "\n",
    "# plot accuracy for various question types\n",
    "plt.bar(range(len(vqaEval.accuracy['perQuestionType'])), vqaEval.accuracy['perQuestionType'].values(), align='center')\n",
    "plt.xticks(range(len(vqaEval.accuracy['perQuestionType'])), vqaEval.accuracy['perQuestionType'].keys(), rotation='0',fontsize=10)\n",
    "plt.title('Per Question Type Accuracy', fontsize=10)\n",
    "plt.xlabel('Question Types', fontsize=10)\n",
    "plt.ylabel('Accuracy', fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# save evaluation results to ./Results folder\n",
    "json.dump(vqaEval.accuracy,     open(accuracyFile,     'w'))\n",
    "json.dump(vqaEval.evalQA,       open(evalQAFile,       'w'))\n",
    "json.dump(vqaEval.evalQuesType, open(evalQuesTypeFile, 'w'))\n",
    "json.dump(vqaEval.evalAnsType,  open(evalAnsTypeFile,  'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
