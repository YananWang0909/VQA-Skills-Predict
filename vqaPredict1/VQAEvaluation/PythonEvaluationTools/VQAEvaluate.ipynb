{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading VQA annotations and questions into memory...\n",
      "0:00:13.527289\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Results do not correspond to current VQA set. Either the results do not have predictions for all question ids in annotation file or there is atleast one question id that does not belong to the question ids in the annotation file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-05ed88c773fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# create vqa object and vqaRes object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mvqa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVQA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquesFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mvqaRes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvqa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadRes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquesFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# create vqaEval object by taking vqa and vqaRes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/programming/VQAEvaluation/PythonHelperTools/vqaTools/vqa.py\u001b[0m in \u001b[0;36mloadRes\u001b[0;34m(self, resFile, quesFile)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mannsQuesIds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mann\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mann\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mann\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                         \u001b[0mquesId\u001b[0m                       \u001b[0;34m=\u001b[0m \u001b[0mann\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'task_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Multiple Choice'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Results do not correspond to current VQA set. Either the results do not have predictions for all question ids in annotation file or there is atleast one question id that does not belong to the question ids in the annotation file."
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import sys\n",
    "dataDir = '/Users/yanan/Desktop/programming/VQAEvaluation'\n",
    "sys.path.insert(0, '%s/PythonHelperTools/vqaTools' %(dataDir))\n",
    "from vqa import VQA\n",
    "sys.path.insert(0, '%s/PythonEvaluationTools' %(dataDir))\n",
    "from vqaEvaluation.vqaEval import VQAEval\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# set up file names and paths\n",
    "versionType ='v2_' # this should be '' when using VQA v2.0 dataset\n",
    "taskType    ='OpenEnded' # 'OpenEnded' only for v2.0. 'OpenEnded' or 'MultipleChoice' for v1.0\n",
    "dataType    ='mscoco'  # 'mscoco' only for v1.0. 'mscoco' for real and 'abstract_v002' for abstract for v1.0. \n",
    "dataSubType ='train2014'\n",
    "annFile     ='%s/Annotations/%s%s_%s_annotations.json'%(dataDir, versionType, dataType, dataSubType)\n",
    "quesFile    ='%s/Questions/%s%s_%s_%s_questions.json'%(dataDir, versionType, taskType, dataType, dataSubType)\n",
    "#imgDir      ='%s/Images/%s/%s/' %(dataDir, dataType, dataSubType)\n",
    "resultType  ='color_des_domin'\n",
    "fileTypes   = ['results', 'accuracy', 'evalQA', 'evalQuesType', 'evalAnsType'] \n",
    "\n",
    "# An example result json file has been provided in './Results' folder.  \n",
    "\n",
    "[resFile, accuracyFile, evalQAFile, evalQuesTypeFile, evalAnsTypeFile] = ['%s/Results/%s%s_%s_%s_%s_%s.json'%(dataDir, versionType, taskType, dataType, dataSubType, \\\n",
    "resultType, fileType) for fileType in fileTypes]  \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# create vqa object and vqaRes object\n",
    "vqa = VQA(annFile, quesFile)\n",
    "vqaRes = vqa.loadRes(resFile, quesFile)\n",
    "\n",
    "# create vqaEval object by taking vqa and vqaRes\n",
    "vqaEval = VQAEval(vqa, vqaRes, n=2)   #n is precision of accuracy (number of places after decimal), default is 2\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "with open(resFile, 'r') as f:\n",
    "    data = json.load(f)\n",
    "data_index=[]\n",
    "for i in range(0,len(data)):\n",
    "    data_index.append(data[i][\"question_id\"])\n",
    "    \n",
    "# evaluate results\n",
    "\"\"\"\n",
    "If you have a list of question ids on which you would like to evaluate your results, pass it as a list to below function\n",
    "By default it uses all the question ids in annotation file\n",
    "\"\"\"\n",
    "\n",
    "vqaEval.evaluate(data_index) \n",
    "\n",
    "# print accuracies\n",
    "print(\"\\n\")\n",
    "print(\"Overall Accuracy is: %.02f\\n\" %(vqaEval.accuracy['overall']))\n",
    "print(\"Per Question Type Accuracy is the following:\")\n",
    "for quesType in vqaEval.accuracy['perQuestionType']:\n",
    "\tprint(\"%s : %.02f\" %(quesType, vqaEval.accuracy['perQuestionType'][quesType]))\n",
    "print(\"\\n\")\n",
    "print(\"Per Answer Type Accuracy is the following:\")\n",
    "for ansType in vqaEval.accuracy['perAnswerType']:\n",
    "\tprint(\"%s : %.02f\" %(ansType, vqaEval.accuracy['perAnswerType'][ansType]))\n",
    "print(\"\\n\")\n",
    "# demo how to use evalQA to retrieve low score result\n",
    "evals = [quesId for quesId in vqaEval.evalQA if vqaEval.evalQA[quesId]<35]   #35 is per question percentage accuracy\n",
    "if len(evals) > 0:\n",
    "\tprint('ground truth answers')\n",
    "\trandomEval = random.choice(evals)\n",
    "\trandomAnn = vqa.loadQA(randomEval)\n",
    "\tvqa.showQA(randomAnn)\n",
    "\n",
    "\tprint('\\n')\n",
    "\tprint('generated answer (accuracy %.02f)'%(vqaEval.evalQA[randomEval]))\n",
    "\tann = vqaRes.loadQA(randomEval)[0]\n",
    "\tprint(\"Answer:   %s\\n\" %(ann['answer']))\n",
    "\n",
    "\t#imgId = randomAnn[0]['image_id']\n",
    "\t#imgFilename = 'COCO_' + dataSubType + '_'+ str(imgId).zfill(12) + '.jpg'\n",
    "\t#if os.path.isfile(imgDir + imgFilename):\n",
    "\t\t#I = io.imread(imgDir + imgFilename)\n",
    "\t\t#plt.imshow(I)\n",
    "\t\t#plt.axis('off')\n",
    "\t\t#plt.show()\n",
    "\n",
    "# plot accuracy for various question types\n",
    "plt.bar(range(len(vqaEval.accuracy['perQuestionType'])), vqaEval.accuracy['perQuestionType'].values(), align='center')\n",
    "plt.xticks(range(len(vqaEval.accuracy['perQuestionType'])), vqaEval.accuracy['perQuestionType'].keys(), rotation='0',fontsize=10)\n",
    "plt.title('Per Question Type Accuracy', fontsize=10)\n",
    "plt.xlabel('Question Types', fontsize=10)\n",
    "plt.ylabel('Accuracy', fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# save evaluation results to ./Results folder\n",
    "json.dump(vqaEval.accuracy,     open(accuracyFile,     'w'))\n",
    "json.dump(vqaEval.evalQA,       open(evalQAFile,       'w'))\n",
    "json.dump(vqaEval.evalQuesType, open(evalQuesTypeFile, 'w'))\n",
    "json.dump(vqaEval.evalAnsType,  open(evalAnsTypeFile,  'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "cat\n",
      "rabbit\n",
      "bee\n",
      "cat\n",
      "['cat', 'cat']\n"
     ]
    }
   ],
   "source": [
    "b=['dog','cat','rabbit','bee','cat']\n",
    "a=['dog','dog','cat','cat','cow','chick','bee']\n",
    "for gtAnsDatum in b:\n",
    "    print(gtAnsDatum)\n",
    "    otherGTAns = [item for item in a if item==gtAnsDatum]\n",
    "\n",
    "print(otherGTAns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
