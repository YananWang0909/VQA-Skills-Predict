{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the prototype, we focus on \"text recognition\" only \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "kfold=KFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>IMG</th>\n",
       "      <th>QSN</th>\n",
       "      <th>ANS_TYP</th>\n",
       "      <th>TXT</th>\n",
       "      <th>OBJ</th>\n",
       "      <th>COL</th>\n",
       "      <th>CNT</th>\n",
       "      <th>OTH</th>\n",
       "      <th>ANS1</th>\n",
       "      <th>ANS2</th>\n",
       "      <th>ANS3</th>\n",
       "      <th>ANS4</th>\n",
       "      <th>ANS5</th>\n",
       "      <th>ANS6</th>\n",
       "      <th>ANS7</th>\n",
       "      <th>ANS8</th>\n",
       "      <th>ANS9</th>\n",
       "      <th>ANS10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VizWiz_train_000000000000.jpg</td>\n",
       "      <td>VizWiz_train_000000000000.jpg</td>\n",
       "      <td>What's the name of this product?</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>basil leaves</td>\n",
       "      <td>basil leaves</td>\n",
       "      <td>basil</td>\n",
       "      <td>basil</td>\n",
       "      <td>basil leaves</td>\n",
       "      <td>basil leaves</td>\n",
       "      <td>basil leaves</td>\n",
       "      <td>basil leaves</td>\n",
       "      <td>basil leaves</td>\n",
       "      <td>basil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VizWiz_train_000000000001.jpg</td>\n",
       "      <td>VizWiz_train_000000000001.jpg</td>\n",
       "      <td>Can you tell me what is in this can please?</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>soda</td>\n",
       "      <td>coca cola</td>\n",
       "      <td>coca cola</td>\n",
       "      <td>unsuitable</td>\n",
       "      <td>unsuitable</td>\n",
       "      <td>coke 0</td>\n",
       "      <td>coca cola 0</td>\n",
       "      <td>coke 0</td>\n",
       "      <td>coca cola</td>\n",
       "      <td>coke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VizWiz_train_000000000002.jpg</td>\n",
       "      <td>VizWiz_train_000000000002.jpg</td>\n",
       "      <td>Is this enchilada sauce or is this tomatoes?  ...</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>these tomatoes not enchilada sauce</td>\n",
       "      <td>tomatoes</td>\n",
       "      <td>tomatoes</td>\n",
       "      <td>tomatoes</td>\n",
       "      <td>tomatoes</td>\n",
       "      <td>crushed tomatoes</td>\n",
       "      <td>crushed tomatoes</td>\n",
       "      <td>tomatoes</td>\n",
       "      <td>tomatoes</td>\n",
       "      <td>tomatoes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             QID                            IMG  \\\n",
       "0  VizWiz_train_000000000000.jpg  VizWiz_train_000000000000.jpg   \n",
       "1  VizWiz_train_000000000001.jpg  VizWiz_train_000000000001.jpg   \n",
       "2  VizWiz_train_000000000002.jpg  VizWiz_train_000000000002.jpg   \n",
       "\n",
       "                                                 QSN ANS_TYP  TXT  OBJ  COL  \\\n",
       "0                   What's the name of this product?   other    1    1    0   \n",
       "1        Can you tell me what is in this can please?   other    1    1    0   \n",
       "2  Is this enchilada sauce or is this tomatoes?  ...   other    1    1    0   \n",
       "\n",
       "   CNT  OTH                                ANS1          ANS2       ANS3  \\\n",
       "0    0    0                        basil leaves  basil leaves      basil   \n",
       "1    0    0                                soda     coca cola  coca cola   \n",
       "2    0    0  these tomatoes not enchilada sauce      tomatoes   tomatoes   \n",
       "\n",
       "         ANS4          ANS5              ANS6              ANS7          ANS8  \\\n",
       "0       basil  basil leaves      basil leaves      basil leaves  basil leaves   \n",
       "1  unsuitable    unsuitable            coke 0       coca cola 0        coke 0   \n",
       "2    tomatoes      tomatoes  crushed tomatoes  crushed tomatoes      tomatoes   \n",
       "\n",
       "           ANS9     ANS10  \n",
       "0  basil leaves     basil  \n",
       "1     coca cola      coke  \n",
       "2      tomatoes  tomatoes  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading in the CSV files as Pandas dataframes\n",
    "df_train = pd.read_csv('vizwiz_skill_typ_train.csv', skipinitialspace = True, engine='python')\n",
    "df_val = pd.read_csv('vizwiz_skill_typ_val.csv', skipinitialspace = True, engine='python')\n",
    "# The structure of VizWiz file\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create X and Ys\n",
    "ques_train=df_train[\"QSN\"].values\n",
    "txt_train=df_train[\"TXT\"].values\n",
    "obj_train=df_train[\"OBJ\"].values\n",
    "col_train=df_train[\"COL\"].values\n",
    "cnt_train=df_train[\"CNT\"].values\n",
    "\n",
    "ques_val=df_val[\"QSN\"].values\n",
    "txt_val=df_val[\"TXT\"].values\n",
    "obj_val=df_val[\"OBJ\"].values\n",
    "col_val=df_val[\"COL\"].values\n",
    "cnt_val=df_val[\"CNT\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using CNTK backend\n"
     ]
    }
   ],
   "source": [
    "#Use LSTM to predict\n",
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "\n",
    "os.environ['KERAS_BACKEND']='cntk'\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.callbacks import History, CSVLogger\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DOC_LEN = 40\n",
    "VOCAB_SIZE = 3000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize, create seqs, pad\n",
    "tok = Tokenizer(num_words=VOCAB_SIZE, lower=True, split=\" \")\n",
    "tok.fit_on_texts(ques_train)\n",
    "train_seq = tok.texts_to_sequences(ques_train)\n",
    "train_seq = sequence.pad_sequences(train_seq, maxlen=MAX_DOC_LEN)\n",
    "val_seq = tok.texts_to_sequences(ques_val)\n",
    "val_seq = sequence.pad_sequences(val_seq, maxlen=MAX_DOC_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews by class in training set\n",
      "[6252. 8017.]\n"
     ]
    }
   ],
   "source": [
    "labels = to_categorical(np.asarray(txt_train))\n",
    "labels = labels.astype('float32')\n",
    "print('Number of reviews by class in training set')\n",
    "print(labels.sum(axis=0))\n",
    "n_classes = labels.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/lovelydf/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "sent_lst = []\n",
    "\n",
    "for doc in ques_train:\n",
    "    sentences = nltk.tokenize.sent_tokenize(doc)\n",
    "    for sent in sentences:\n",
    "        word_lst = [w for w in nltk.tokenize.word_tokenize(sent) if w.isalnum()]\n",
    "        sent_lst.append(word_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-13 07:20:03,691 : INFO : collecting all words and their counts\n",
      "2018-11-13 07:20:03,692 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-11-13 07:20:03,703 : INFO : PROGRESS: at sentence #10000, processed 54533 words, keeping 2478 word types\n",
      "2018-11-13 07:20:03,711 : INFO : collected 3155 word types from a corpus of 88953 raw words and 16228 sentences\n",
      "2018-11-13 07:20:03,712 : INFO : Loading a fresh vocabulary\n",
      "2018-11-13 07:20:03,715 : INFO : effective_min_count=6 retains 749 unique words (23% of original 3155, drops 2406)\n",
      "2018-11-13 07:20:03,716 : INFO : effective_min_count=6 leaves 84885 word corpus (95% of original 88953, drops 4068)\n",
      "2018-11-13 07:20:03,718 : INFO : deleting the raw counts dictionary of 3155 items\n",
      "2018-11-13 07:20:03,719 : INFO : sample=0.001 downsamples 44 most-common words\n",
      "2018-11-13 07:20:03,720 : INFO : downsampling leaves estimated 35348 word corpus (41.6% of prior 84885)\n",
      "2018-11-13 07:20:03,722 : INFO : estimated required memory for 749 words and 100 dimensions: 973700 bytes\n",
      "2018-11-13 07:20:03,723 : INFO : resetting layer weights\n",
      "2018-11-13 07:20:03,735 : INFO : training model with 6 workers on 749 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-11-13 07:20:03,779 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-13 07:20:03,780 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-13 07:20:03,786 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-13 07:20:03,790 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-13 07:20:03,792 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-13 07:20:03,793 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-13 07:20:03,793 : INFO : EPOCH - 1 : training on 88953 raw words (35406 effective words) took 0.0s, 709820 effective words/s\n",
      "2018-11-13 07:20:03,838 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-13 07:20:03,840 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-13 07:20:03,841 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-13 07:20:03,845 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-13 07:20:03,850 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-13 07:20:03,853 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-13 07:20:03,853 : INFO : EPOCH - 2 : training on 88953 raw words (35467 effective words) took 0.1s, 694395 effective words/s\n",
      "2018-11-13 07:20:03,898 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-13 07:20:03,901 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-13 07:20:03,902 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-13 07:20:03,903 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-13 07:20:03,910 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-13 07:20:03,911 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-13 07:20:03,912 : INFO : EPOCH - 3 : training on 88953 raw words (35579 effective words) took 0.0s, 732851 effective words/s\n",
      "2018-11-13 07:20:03,955 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-13 07:20:03,958 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-13 07:20:03,959 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-13 07:20:03,963 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-13 07:20:03,966 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-13 07:20:03,967 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-13 07:20:03,968 : INFO : EPOCH - 4 : training on 88953 raw words (35188 effective words) took 0.0s, 743877 effective words/s\n",
      "2018-11-13 07:20:04,013 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-11-13 07:20:04,016 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-11-13 07:20:04,018 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-13 07:20:04,023 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-13 07:20:04,027 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-13 07:20:04,028 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-13 07:20:04,029 : INFO : EPOCH - 5 : training on 88953 raw words (35582 effective words) took 0.1s, 695531 effective words/s\n",
      "2018-11-13 07:20:04,030 : INFO : training on a 444765 raw words (177222 effective words) took 0.3s, 603083 effective words/s\n",
      "2018-11-13 07:20:04,030 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "import gensim, logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# use skip-gram\n",
    "word2vec_model = gensim.models.Word2Vec(sentences=sent_lst, min_count=6, size=EMBEDDING_DIM, sg=1, workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 749 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "for word in word2vec_model.wv.vocab:\n",
    "    coefs = np.asarray(word2vec_model.wv[word], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# Initial embedding\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in tok.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None and i < VOCAB_SIZE:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "NUM_EPOCHS = 30\n",
    "LSTM_DIM = 100\n",
    "OPTIMIZER = SGD(lr=0.01, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_create_train(reg_param):\n",
    "    l2_reg = regularizers.l2(reg_param)\n",
    "\n",
    "    # model init\n",
    "    embedding_layer = Embedding(VOCAB_SIZE,\n",
    "                                EMBEDDING_DIM,\n",
    "                                input_length=MAX_DOC_LEN,\n",
    "                                trainable=True,\n",
    "                                mask_zero=False,\n",
    "                                embeddings_regularizer=l2_reg,\n",
    "                                weights=[embedding_matrix])\n",
    "\n",
    "    lstm_layer = LSTM(units=LSTM_DIM, kernel_regularizer=l2_reg)\n",
    "    dense_layer = Dense(n_classes, activation='softmax', kernel_regularizer=l2_reg)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Bidirectional(lstm_layer))\n",
    "    model.add(dense_layer)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=OPTIMIZER,\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    fname = \"lstm\"\n",
    "    history = History()\n",
    "    csv_logger = CSVLogger('./{0}_{1}.log'.format(fname, reg_param),\n",
    "                           separator=',',\n",
    "                           append=True)\n",
    "\n",
    "    t1 = time.time()\n",
    "    # model fit\n",
    "    model.fit(train_seq,\n",
    "              labels.astype('float32'),\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=NUM_EPOCHS,\n",
    "              callbacks=[history, csv_logger],\n",
    "              verbose=2)\n",
    "    t2 = time.time()\n",
    "\n",
    "    # save model\n",
    "    model.save('./{0}_{1}_model.h5'.format(fname, reg_param))\n",
    "    np.savetxt('./{0}_{1}_time.txt'.format(fname, reg_param), \n",
    "               [reg_param, (t2-t1) / 3600])\n",
    "    with open('./{0}_{1}_history.txt'.format(fname, reg_param), \"w\") as res_file:\n",
    "        res_file.write(str(history.history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 12s - loss: 0.6838 - acc: 0.5590\n",
      "Epoch 2/30\n",
      " - 11s - loss: 0.6772 - acc: 0.5637\n",
      "Epoch 3/30\n",
      " - 11s - loss: 0.6716 - acc: 0.5721\n",
      "Epoch 4/30\n",
      " - 11s - loss: 0.6656 - acc: 0.5781\n",
      "Epoch 5/30\n",
      " - 11s - loss: 0.6597 - acc: 0.5857\n",
      "Epoch 6/30\n",
      " - 11s - loss: 0.6533 - acc: 0.6024\n",
      "Epoch 7/30\n",
      " - 11s - loss: 0.6468 - acc: 0.6281\n",
      "Epoch 8/30\n",
      " - 11s - loss: 0.6401 - acc: 0.6651\n",
      "Epoch 9/30\n",
      " - 11s - loss: 0.6333 - acc: 0.6729\n",
      "Epoch 10/30\n",
      " - 11s - loss: 0.6261 - acc: 0.6778\n",
      "Epoch 11/30\n",
      " - 11s - loss: 0.6186 - acc: 0.6801\n",
      "Epoch 12/30\n",
      " - 11s - loss: 0.6108 - acc: 0.6841\n",
      "Epoch 13/30\n",
      " - 11s - loss: 0.6027 - acc: 0.6883\n",
      "Epoch 14/30\n",
      " - 11s - loss: 0.5941 - acc: 0.6919\n",
      "Epoch 15/30\n",
      " - 11s - loss: 0.5856 - acc: 0.6934\n",
      "Epoch 16/30\n",
      " - 11s - loss: 0.5774 - acc: 0.6973\n",
      "Epoch 17/30\n",
      " - 11s - loss: 0.5699 - acc: 0.6988\n",
      "Epoch 18/30\n",
      " - 11s - loss: 0.5625 - acc: 0.7031\n",
      "Epoch 19/30\n",
      " - 11s - loss: 0.5557 - acc: 0.7050\n",
      "Epoch 20/30\n",
      " - 11s - loss: 0.5485 - acc: 0.7088\n",
      "Epoch 21/30\n",
      " - 11s - loss: 0.5425 - acc: 0.7125\n",
      "Epoch 22/30\n",
      " - 11s - loss: 0.5352 - acc: 0.7161\n",
      "Epoch 23/30\n",
      " - 11s - loss: 0.5302 - acc: 0.7189\n",
      "Epoch 24/30\n",
      " - 11s - loss: 0.5253 - acc: 0.7197\n",
      "Epoch 25/30\n",
      " - 11s - loss: 0.5194 - acc: 0.7255\n",
      "Epoch 26/30\n",
      " - 11s - loss: 0.5153 - acc: 0.7302\n",
      "Epoch 27/30\n",
      " - 11s - loss: 0.5106 - acc: 0.7319\n",
      "Epoch 28/30\n",
      " - 11s - loss: 0.5055 - acc: 0.7370\n",
      "Epoch 29/30\n",
      " - 11s - loss: 0.5029 - acc: 0.7406\n",
      "Epoch 30/30\n",
      " - 11s - loss: 0.4996 - acc: 0.7427\n"
     ]
    }
   ],
   "source": [
    "lstm_create_train(1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7721012883163039 \t AUC = 0.8429309852839264\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "model = load_model('./lstm_{0}_model.h5'.format(1e-7))\n",
    "preds = model.predict(val_seq, verbose=0)\n",
    "print((\"Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(txt_val, preds.argmax(axis=1)), \n",
    "       roc_auc_score(txt_val, preds[:, 1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
